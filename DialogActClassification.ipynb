{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DialogActClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuragse0012/DialogActClassification/blob/master/DialogActClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LaYpPDPIyLzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQIiqn3zLoWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvj1A1xO92z2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6sNcwNm93qJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls /content/drive/My Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qorQI61gxinA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Add, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras.callbacks import History\n",
        "from keras.models import load_model\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def trunc_at(s, d, n=3):\n",
        "    \"Returns s truncated at the n'th (3rd by default) occurrence of the delimiter, d.\"\n",
        "    return \"\".join(s.split(d, n)[n:n+1]).strip()    ## return \"\".join(s.split(d, n)[n-3:n+1]).strip() --> this return full context utterances\n",
        "\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "# defining list of the classes\n",
        "\n",
        "label_list = ['%','%--','2','aa','aap','ar','b','ba','bc','bd','bh','bk','br','bs','cc','co','d','fa','ft','g','h','no','qh','qo','qrr','qw','qy','s','t1','t3','x']\n",
        "\n",
        "#-------loading and preprocessing the training data ---------#\n",
        "\n",
        "# ----------------------Start--------------------------------------------- #\n",
        "data_train = pd.read_csv(\"/content/drive/My Drive/utterances.train\", sep='\\t', header=None)\n",
        "\n",
        "trained_Data = []\n",
        "text_train = data_train[data_train.columns[2]].tolist()\n",
        "\n",
        "\n",
        "for data in text_train:\n",
        "    trun_data = trunc_at(data,\";\")\n",
        "    trained_Data.append(trun_data)\n",
        "\n",
        "\n",
        "labels_train = data_train[int(data_train.columns[1])].tolist()\n",
        "\n",
        "#----------------- End -----------------------------------------------#\n",
        "\n",
        "#-------loading and preprocessing the development data ---------#\n",
        "\n",
        "# ----------------------Start--------------------------------------------- #\n",
        "data_dev = pd.read_csv(\"/content/drive/My Drive/utterances.valid\",sep='\\t', header=None)\n",
        "text_dev = data_dev[data_dev.columns[2]].tolist()\n",
        "\n",
        "developed_data = []\n",
        "\n",
        "for data in text_dev:\n",
        "    trun_data = trunc_at(data,\";\")\n",
        "    developed_data.append(trun_data)\n",
        "\n",
        "labels_dev = data_dev[(data_dev.columns[1])].tolist()\n",
        "\n",
        "with open('/content/drive/My Drive/developed.txt', 'w') as out_file:\n",
        "    for i in range(len(developed_data)):\n",
        "        out_file.write(developed_data[i])\n",
        "        \n",
        "\n",
        "        out_file.write('\\n')\n",
        "\n",
        "\n",
        "#----------------- End -----------------------------------------------#\n",
        "\n",
        "# --- Tokenizing the training data --------------------------------#\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(trained_Data)\n",
        "sequences = tokenizer.texts_to_sequences(trained_Data)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(word_index)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "print(\"word\",len(word_index))\n",
        "\n",
        "word_index['UNK'] = 9754\n",
        "\n",
        "data_test = pd.read_csv(\"/content/drive/My Drive/utterances.test\", sep='\\t',header=None)\n",
        "#print(data_test.shape)\n",
        "\n",
        "text_test = data_test[data_test.columns[1]].tolist()\n",
        "\n",
        "#load the utterances from the test data\n",
        "utterances_id = data_test[data_test.columns[0]].tolist()\n",
        "\n",
        "test_data = []\n",
        "\n",
        "for data in text_test:\n",
        "    trun_data = trunc_at(data,\";\")\n",
        "    test_data.append(trun_data)\n",
        "\n",
        "\n",
        "\n",
        "sequences_test = []           \n",
        "for sentence in test_data:\n",
        "   \n",
        "    words = sentence.lower().split()\n",
        "    \n",
        "    sent = []\n",
        "    for w in words:\n",
        "        #print w\n",
        "        if w not in word_index.keys():\n",
        "            sent.append(word_index['UNK'])\n",
        "        else:\n",
        "            sent.append(word_index[w])\n",
        "    sequences_test.append(sent)   \n",
        "    if len(words) > MAX_SEQUENCE_LENGTH:\n",
        "        MAX_SEQUENCE_LENGTH = len(words)\n",
        "print(\"after test\",MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "sequences_dev = []\n",
        "for sentence in developed_data:\n",
        "    \n",
        "    words = sentence.lower().split()\n",
        "    sent = []\n",
        "    for w in words:\n",
        "        if w not in word_index.keys():\n",
        "            sent.append(word_index['UNK'])\n",
        "        else:\n",
        "            sent.append(word_index[w])\n",
        "    sequences_dev.append(sent)\n",
        "    \n",
        "    if len(words) > MAX_SEQUENCE_LENGTH:\n",
        "        MAX_SEQUENCE_LENGTH = len(words)\n",
        "print(\"after dev\",MAX_SEQUENCE_LENGTH)\n",
        "print(\"length of the sentence\",len(sent))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pad the sequences to the max length\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_dev = pad_sequences(sequences_dev, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "#------ Converting the labels list to binary matrix using encoding ------- #\n",
        "encoder_train = LabelEncoder()\n",
        "encoder_train.fit(labels_train)\n",
        "encoded_trainLabel = encoder_train.transform(labels_train)\n",
        "labels = to_categorical(np.asarray(encoded_trainLabel))\n",
        "\n",
        "encoder_dev = LabelEncoder()\n",
        "encoder_dev.fit(labels_dev)\n",
        "encoded_devLabel = encoder_dev.transform(labels_dev)\n",
        "labels_dev = to_categorical(np.asarray(encoded_devLabel))\n",
        "\n",
        "\n",
        "print('Shape of train_data tensor:', data.shape)\n",
        "print('Shape of train_label tensor:', labels.shape)\n",
        "\n",
        "print('Shape of dev_data tensor:', data_dev.shape)\n",
        "print('Shape of dev_label tensor:', labels_dev.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data\n",
        "\n",
        "y_train = labels\n",
        "\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "\n",
        "#load the whole embedding into the memory\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/My Drive/word2vec_embeddings.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"loaded %s word vectors. \"% len(embeddings_index))\n",
        "\n",
        "#create a weight matrix for the words in training docs\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "\n",
        "print(\"embedding range:\",embedding_matrix.shape)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size,EMBEDDING_DIM , weights=[embedding_matrix], mask_zero=True, input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Bidirectional(LSTM(200)))\n",
        "model.add(Dropout(0.50))\n",
        "\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(31, activation='softmax'))\n",
        "\n",
        "#optimizers : adam, adagrad, sgd, adadelta, adamax, rmsprop\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# fit the model\n",
        "history = model.fit(x_train, y_train, validation_data=(data_dev[:8500], labels_dev[:8500]), shuffle=True, batch_size = 128, epochs=4)\n",
        "\n",
        "#saving the model\n",
        "model.save(\"BiDirectionalLSTM-Model-mini-batch_f\") \n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(data_dev[9000:17000], labels_dev[9000:17000], verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "labels_test = model.predict(data_test)\n",
        "\n",
        "print(type(labels_test))\n",
        "\n",
        "print(range(len(labels_test)))\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/output.txt', 'w') as out_file:\n",
        "    for i in range(len(labels_test)):\n",
        "        out_file.write(utterances_id[i])\n",
        "        out_file.write('  ')\n",
        "        out_file.write(label_list[np.argmax(labels_test[i])])\n",
        "\n",
        "        out_file.write('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40CRgfB2a_g8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"/content/drive/My Drive/utterances.valid\", sep='\\t', header=None)\n",
        "\n",
        "trained_Data = []\n",
        "text_train = data_train[data_train.columns[2]].tolist()\n",
        "\n",
        "for data in text_train:\n",
        "    trun_data = trunc_at(data,\";\")\n",
        "    trained_Data.append(trun_data)\n",
        "\n",
        "with open('/content/drive/My Drive/train.txt', 'w') as out_file:\n",
        "    for i in range(len(trained_Data)):\n",
        "        out_file.write(trained_Data[i])\n",
        "        \n",
        "        out_file.write('\\n')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9MR5vlxkBL5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s = \"Line - out into the line - in on a tape player , ; Mm - hmm . ; Hmm . ; Yep .\"\n",
        "s1 = \";  ;  ; Okay , uh\"\n",
        "def trunc_at(s, d, n=3):\n",
        "    \"Returns s truncated at the n'th (3rd by default) occurrence of the delimiter, d.\"\n",
        "    \n",
        "    return \"\".join(s.split(d, n)[n-3:n+1]).strip()\n",
        "\n",
        "print(trunc_at(s,\";\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SE3hbGFF3Nl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "t = [[4,5,6,7,1,8,4],[6,7,8,9,6,7,8]]\n",
        "\n",
        "print(type(t))\n",
        "\n",
        "for i in range(len(t)):\n",
        "  print(np.argmax(t[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h88BX_Oe-4FB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"/content/drive/My Drive/utterances.train\", sep='\\t', header=None)\n",
        "\n",
        "trained_Data = []\n",
        "text_train = data_train[data_train.columns[2]].tolist()\n",
        "\n",
        "def trunc_at(s, d, n=3):\n",
        "    \"Returns s truncated at the n'th (3rd by default) occurrence of the delimiter, d.\"\n",
        "    return \"\".join(s.split(d, n)[n:n+1]).strip()  ## return \"\".join(s.split(d, n)[n-3:n+1]).strip() --> this return full context utterances\n",
        "\n",
        "for data in text_train[:50]:\n",
        "    trun_data = trunc_at(data,\";\")\n",
        "    trained_Data.append(trun_data)\n",
        "    \n",
        "    \n",
        "for i in range(len(trained_Data)):\n",
        "  print(trained_Data[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}